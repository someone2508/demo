# Save a compact KPI summary for quick reference
summary_kpis = {
    'overall_funnel': funnel_summary.to_dict(orient='records'),
    'top10_deposit_share': float(share_top10),
    'gap_stats': stats_tbl.set_index('metric')['value'].to_dict(),
}
with open(OUTPUT_DIR / 'kpi_summary.json', 'w') as f:
    json.dump(summary_kpis, f, indent=2)
print('KPI summary written to', OUTPUT_DIR / 'kpi_summary.json')### Notes and Interpretations

- Funnel conversion: we compute overall and by `acquisition_channel` and `signup_month` to identify highest drop-offs by stage and segment.
- 30-day activity: approximated by intersecting monthly `ActivePlayerDays` with the first 30 days post-signup; players with any positive overlap are counted as active.
- Cohorts: activity-day buckets show distribution of engagement levels and their deposit contribution.
- Gap (depositâ†’bet): we report mean, median, P75, and max; also provide a bucketed distribution to contrast short vs long gaps.
- Segmentation: top 10% by deposit shows concentration; first-deposit distribution bins plus correlation with 30-day activity suggest whether higher initial deposits are associated with higher early engagement.
- All tables exported as CSV under `output/` and visuals as HTML for easy sharing.# First deposit distribution visuals and clustering discussion

fd = first_deposits_clean[['Src_Player_Id','First_Deposit_Amount']].copy()
fd = fd.dropna(subset=['First_Deposit_Amount'])
fd['First_Deposit_Amount'] = pd.to_numeric(fd['First_Deposit_Amount'], errors='coerce')
fd = fd.dropna()

# Binning into meaningful buckets
bins = [-0.01, 5, 10, 20, 50, 100, 200, 500, 1000, 1e9]
labels = ['<=5','6-10','11-20','21-50','51-100','101-200','201-500','501-1000','>1000']
fd['bucket'] = pd.cut(fd['First_Deposit_Amount'], bins=bins, labels=labels)

bucket_dist = fd['bucket'].value_counts().sort_index().reset_index()
bucket_dist.columns = ['bucket','players']
bucket_dist['share_%'] = (bucket_dist['players']/bucket_dist['players'].sum())*100
bucket_dist.to_csv(OUTPUT_DIR / 'first_deposit_bucket_distribution.csv', index=False)

fig = px.histogram(fd, x='First_Deposit_Amount', nbins=80, title='First Deposit Amount Distribution', log_y=True)
fig.write_html(str(OUTPUT_DIR / 'first_deposit_histogram.html'))

# Profitability proxy: is higher first deposit associated with more 30-day activity?
fd_base = fd.merge(base[['Src_Player_Id','active_days_30d','stage_active_30d']], on='Src_Player_Id', how='left')

corr = fd_base[['First_Deposit_Amount','active_days_30d']].corr().iloc[0,1]
print('Correlation(first_deposit_amount, active_days_30d)=', round(corr,4))

fig2 = px.scatter(fd_base.sample(min(10000, len(fd_base))), x='First_Deposit_Amount', y='active_days_30d', trendline='ols', title='First Deposit vs 30-day Active Days')
fig2.write_html(str(OUTPUT_DIR / 'first_deposit_vs_active_days_scatter.html'))

agg = fd_base.groupby('bucket', dropna=False).agg(
    players=('Src_Player_Id','nunique'),
    avg_first_dep=('First_Deposit_Amount','mean'),
    active_rate=('stage_active_30d','mean'),
    avg_active_days=('active_days_30d','mean')
).reset_index()
agg.to_csv(OUTPUT_DIR / 'first_deposit_bucket_engagement.csv', index=False)
agg.head()# Player segmentation: Top 10% by total deposit and share

# We only have first deposit amount per player in this dataset. We'll treat it as total deposit proxy.
player_dep = base[['Src_Player_Id','total_deposit_amount']].copy()
player_dep = player_dep.groupby('Src_Player_Id', as_index=False)['total_deposit_amount'].sum()

player_dep = player_dep.sort_values('total_deposit_amount', ascending=False)
player_dep['rank'] = np.arange(1, len(player_dep)+1)
player_dep['percentile'] = player_dep['rank'] / len(player_dep)

cutoff_idx = int(math.ceil(0.10 * len(player_dep)))
Top10 = player_dep.head(cutoff_idx)
share_top10 = Top10['total_deposit_amount'].sum() / player_dep['total_deposit_amount'].sum()

summary = pd.DataFrame({
    'metric':['num_players','top10_share_of_deposits'],
    'value':[len(player_dep), share_top10]
})

summary.to_csv(OUTPUT_DIR / 'top10_segmentation_summary.csv', index=False)
Top10[['Src_Player_Id','total_deposit_amount']].to_csv(OUTPUT_DIR / 'top10_players_by_deposit.csv', index=False)

print(summary)
print('Top 10% deposit share:', round(share_top10*100,2), '%')# Gap between first deposit and first bet

gap_df = base.dropna(subset=['First_Deposit_Date', 'System_First_Bet_Datetime']).copy()

# Only consider gaps >= 0 (bet after deposit); allow same-day positive or zero
mask = (gap_df['System_First_Bet_Datetime'] >= gap_df['First_Deposit_Date'])

gap_df = gap_df[mask].copy()

gap_df['days_deposit_to_bet'] = (gap_df['System_First_Bet_Datetime'] - gap_df['First_Deposit_Date']).dt.total_seconds() / (24*3600)

# Descriptive stats
mean_days = gap_df['days_deposit_to_bet'].mean()
median_days = gap_df['days_deposit_to_bet'].median()
p75 = gap_df['days_deposit_to_bet'].quantile(0.75)
max_days = gap_df['days_deposit_to_bet'].max()

stats_tbl = pd.DataFrame({
    'metric':['mean','median','p75','max'],
    'value':[mean_days, median_days, p75, max_days]
})

stats_tbl.to_csv(OUTPUT_DIR / 'gap_deposit_to_bet_stats.csv', index=False)

# Distribution shape: proportion with short gaps vs long gaps
bins = [-0.01, 0, 1, 3, 7, 14, 30, 60, 180, 1e9]
labels = ['same_day','1d','2-3d','4-7d','8-14d','15-30d','31-60d','61-180d','>180d']

hist = pd.cut(gap_df['days_deposit_to_bet'], bins=bins, labels=labels)
dist_tbl = hist.value_counts(dropna=False).reset_index()
dist_tbl.columns = ['gap_bucket','players']
dist_tbl['share_%'] = (dist_tbl['players']/dist_tbl['players'].sum())*100

dist_tbl.to_csv(OUTPUT_DIR / 'gap_deposit_to_bet_distribution.csv', index=False)

print(stats_tbl)
print(dist_tbl.head(10))# Retention & Engagement: 30-day active days already computed in base
# Cohorts: by signup_day buckets (1, 2-3, 5-6, etc.)

# Define cohorts by number of active days buckets
bins = [-0.1, 0, 1, 2, 3, 5, 6, 10, 30]
labels = ['0','1','2','3','4-5','6','7-10','11-30']

base['active_days_bucket'] = pd.cut(base['active_days_30d'], bins=bins, labels=labels, right=True)

cohort_counts = base.groupby('active_days_bucket', dropna=False)['Src_Player_Id'].nunique().reset_index(name='players')
cohort_counts['share_%'] = (cohort_counts['players'] / cohort_counts['players'].sum()) * 100
cohort_counts.to_csv(OUTPUT_DIR / 'active_days_cohorts.csv', index=False)
print(cohort_counts)

# Which cohort contributes most to total deposits?
# Build player-level total deposits using first deposit + any inferred top-ups (not provided). We'll use First_Deposit_Amount as proxy.
player_deposit = first_deposits_clean.groupby('Src_Player_Id', as_index=False)['First_Deposit_Amount'].sum().rename(columns={'First_Deposit_Amount':'total_deposit_amount'})
base = base.merge(player_deposit, on='Src_Player_Id', how='left')
base['total_deposit_amount'] = base['total_deposit_amount'].fillna(0)

deposit_by_cohort = base.groupby('active_days_bucket', dropna=False)['total_deposit_amount'].sum().reset_index()
deposit_by_cohort['share_%'] = (deposit_by_cohort['total_deposit_amount'] / deposit_by_cohort['total_deposit_amount'].sum()) * 100
deposit_by_cohort.to_csv(OUTPUT_DIR / 'deposit_contribution_by_active_cohort.csv', index=False)
print(deposit_by_cohort.sort_values('total_deposit_amount', ascending=False).head(10))# Compute 30-day active days per player using month overlap approximation
from pandas.tseries.offsets import MonthBegin, MonthEnd

act = activity_clean[['Src_Player_Id','ActivityMonth','ActivePlayerDays','Bet_Amount','Win_Amount','Gross_Win','Net_Gross_Win']].copy()
act = act.dropna(subset=['Src_Player_Id'])
act['ActivityMonth'] = pd.to_datetime(act['ActivityMonth'], errors='coerce')
act['month_start'] = act['ActivityMonth'].dt.to_period('M').dt.to_timestamp(how='start')
act['month_end'] = act['ActivityMonth'].dt.to_period('M').dt.to_timestamp(how='end')

signups = players_clean[['Src_Player_Id','Signup_Date']].dropna()
merged = act.merge(signups, on='Src_Player_Id', how='inner')

window_start = merged['Signup_Date']
window_end = merged['Signup_Date'] + pd.Timedelta(days=30)

# Row-wise overlap between [month_start, month_end] and [window_start, window_end)
start = np.where(merged['month_start'] > window_start, merged['month_start'], window_start)
end = np.where(merged['month_end'] < window_end, merged['month_end'], window_end)

# Compute inclusive days overlap; clip negatives to 0
overlap_days = (pd.to_datetime(end) - pd.to_datetime(start)).astype('timedelta64[D]').astype(float)
overlap_days = np.where(overlap_days < 0, 0, overlap_days)

# Conservative approximation: active days within window cannot exceed overlap_days or reported ActivePlayerDays
window_active_days = np.minimum(merged['ActivePlayerDays'].fillna(0).to_numpy(dtype=float), overlap_days)

active_30d = (
    pd.DataFrame({'Src_Player_Id': merged['Src_Player_Id'].values, 'active_days_30d': window_active_days})
      .groupby('Src_Player_Id', as_index=False)['active_days_30d'].sum()
)

# Attach 30d activity to base table
base = base.merge(active_30d, on='Src_Player_Id', how='left')
base['active_days_30d'] = base['active_days_30d'].fillna(0)
base['active_30d_flag'] = base['active_days_30d'] > 0

# Funnel stages
base['stage_registration'] = True
base['stage_first_deposit'] = base['First_Deposit_Date'].notna()
base['stage_first_bet'] = base['System_First_Bet_Datetime'].notna()
base['stage_active_30d'] = base['active_30d_flag']

funnel_counts = pd.Series({
    'Registrations': int(base['stage_registration'].sum()),
    'First_Deposit': int(base['stage_first_deposit'].sum()),
    'First_Bet': int(base['stage_first_bet'].sum()),
    'Active_30d': int(base['stage_active_30d'].sum()),
})

# Conversion relative to previous stage
conv_dep = funnel_counts['First_Deposit'] / funnel_counts['Registrations']
conv_bet = base.loc[base['stage_first_deposit'], 'stage_first_bet'].mean()
conv_active = base.loc[base['stage_first_bet'], 'stage_active_30d'].mean() if funnel_counts['First_Bet']>0 else np.nan

funnel_summary = pd.DataFrame({
    'stage': ['Registrations','First_Deposit','First_Bet','Active_30d'],
    'count': funnel_counts.values,
    'conversion_from_prev': [np.nan, conv_dep, conv_bet, conv_active]
})

funnel_summary.to_csv(OUTPUT_DIR / 'funnel_overall.csv', index=False)
print(funnel_summary)

# By acquisition channel and signup cohort (month)

def funnel_by_group(df: pd.DataFrame, group_cols: list) -> pd.DataFrame:
    g = df.groupby(group_cols)
    out = []
    for keys, sub in g:
        regs = len(sub)
        dep = int(sub['stage_first_deposit'].sum())
        bet = int(sub['stage_first_bet'].sum())
        act30 = int(sub['stage_active_30d'].sum())
        conv_dep = dep / regs if regs else np.nan
        conv_bet = sub.loc[sub['stage_first_deposit'], 'stage_first_bet'].mean() if dep else np.nan
        conv_active = sub.loc[sub['stage_first_bet'], 'stage_active_30d'].mean() if bet else np.nan
        row = {'count': regs, 'First_Deposit': dep, 'First_Bet': bet, 'Active_30d': act30,
               'conv_dep': conv_dep, 'conv_bet': conv_bet, 'conv_active': conv_active}
        if isinstance(keys, tuple):
            for c, k in zip(group_cols, keys):
                row[c] = k
        else:
            row[group_cols[0]] = keys
        out.append(row)
    return pd.DataFrame(out).sort_values('count', ascending=False)

funnel_by_channel = funnel_by_group(base, ['acquisition_channel'])
funnel_by_channel.to_csv(OUTPUT_DIR / 'funnel_by_acquisition_channel.csv', index=False)

funnel_by_cohort = funnel_by_group(base, ['signup_month'])
funnel_by_cohort.to_csv(OUTPUT_DIR / 'funnel_by_signup_month.csv', index=False)

print(funnel_by_channel.head())
print(funnel_by_cohort.head())# Filter to non-internal players and basic cleansing
players_clean = players.copy()
if 'Internal_Player_YN' in players_clean.columns:
    players_clean = players_clean[players_clean['Internal_Player_YN'].fillna('N') != 'Y']
players_clean['acquisition_channel'] = players_clean.get('acquisition_channel', 'Unknown').fillna('Unknown')
players_clean['signup_month'] = players_clean['Signup_Date'].dt.to_period('M').astype(str)

first_deposits_clean = first_deposits.copy()
first_deposits_clean['First_Deposit_Amount'] = pd.to_numeric(first_deposits_clean['First_Deposit_Amount'], errors='coerce')

first_bets_clean = first_bets.copy()

activity_clean = activity.copy()
activity_clean['ActivePlayerDays'] = pd.to_numeric(activity_clean['ActivePlayerDays'], errors='coerce')
activity_clean['Bet_Amount'] = pd.to_numeric(activity_clean['Bet_Amount'], errors='coerce')
activity_clean['Win_Amount'] = pd.to_numeric(activity_clean['Win_Amount'], errors='coerce')
activity_clean['Gross_Win'] = pd.to_numeric(activity_clean['Gross_Win'], errors='coerce')
activity_clean['Net_Gross_Win'] = pd.to_numeric(activity_clean['Net_Gross_Win'], errors='coerce')

bonus_clean = bonus.copy()
bonus_clean['BONUS_COST'] = pd.to_numeric(bonus_clean['BONUS_COST'], errors='coerce')

# One row per player across the master tables
base = players_clean[['Src_Player_Id','Signup_Date','acquisition_channel','signup_month']].drop_duplicates()
base = base.merge(first_deposits_clean[['Src_Player_Id','First_Deposit_Date','First_Deposit_Amount','First_Deposit_Channel','First_Deposit_Method']], on='Src_Player_Id', how='left')
base = base.merge(first_bets_clean[['Src_Player_Id','System_First_Bet_Datetime','System_First_BetSlip_Amt','System_First_Bet_Product_Group','System_First_Bet_Product']], on='Src_Player_Id', how='left')

print('Base rows (unique players, non-internal):', len(base))
base.head()# Imports & Config
import os
import math
import json
from pathlib import Path
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

pd.set_option('display.max_columns', 200)
pd.set_option('display.max_rows', 50)

DATA_DIR = Path('/workspace/data')
OUTPUT_DIR = Path('/workspace/output')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Helper: read excel with headers on row 2

def read_backend(path: Path, sheet_preference=('Backend Data', 'Sheet1')) -> pd.DataFrame:
    xls = pd.ExcelFile(path)
    chosen = None
    for s in sheet_preference:
        if s in xls.sheet_names:
            chosen = s
            break
    if chosen is None:
        chosen = xls.sheet_names[0]
    df = pd.read_excel(path, sheet_name=chosen, header=1)
    # Drop unnamed index column if present
    if 'Unnamed: 0' in df.columns:
        df = df.drop(columns=['Unnamed: 0'])
    return df

players = read_backend(DATA_DIR / 'Player_Details.xlsx')
first_bets = read_backend(DATA_DIR / 'First_Bet_Data.xlsx', sheet_preference=('Sheet1',))
first_deposits = read_backend(DATA_DIR / 'First_Deposit_Data.xlsx')
activity = read_backend(DATA_DIR / 'Player_Activity_Data.xlsx')
bonus = read_backend(DATA_DIR / 'BonusCost_Data.xlsx')

# Standardize column names to snake_case

def clean_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip().replace(' ', '_').replace('-', '_') for c in df.columns]
    return df

players = clean_cols(players)
first_bets = clean_cols(first_bets)
first_deposits = clean_cols(first_deposits)
activity = clean_cols(activity)
bonus = clean_cols(bonus)

players.rename(columns={'src_player_id':'Src_Player_Id'}, inplace=True)
activity.rename(columns={'src_player_id':'Src_Player_Id'}, inplace=True)
bonus.rename(columns={'src_player_id':'Src_Player_Id','Src_PLAYER_ID':'Src_Player_Id'}, inplace=True)

# Parse dates
for col in ['Signup_Date','Date_Of_Birth']:
    if col in players.columns:
        players[col] = pd.to_datetime(players[col], errors='coerce')

for col in ['System_First_Bet_Datetime']:
    if col in first_bets.columns:
        first_bets[col] = pd.to_datetime(first_bets[col], errors='coerce')

for col in ['First_Deposit_Date']:
    if col in first_deposits.columns:
        first_deposits[col] = pd.to_datetime(first_deposits[col], errors='coerce')

for col in ['ActivityMonth']:
    if col in activity.columns:
        activity[col] = pd.to_datetime(activity[col], errors='coerce')

print('Shapes:')
print('players', players.shape)
print('first_deposits', first_deposits.shape)
print('first_bets', first_bets.shape)
print('activity', activity.shape)
print('bonus', bonus.shape)## Assignment Analysis: Funnel, Retention, and Segmentation

This notebook builds the full analysis required by the assignment using the provided datasets. It loads data, cleans/validates, computes KPIs, and renders tables/charts with succinct interpretations for each task.